{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手写梯度下降算法及其他相关类型的优化器迭代算法来巩固知识\n",
    "\n",
    "## 1 gradient_descent\n",
    "梯度下降算法是神经网络模型训练最常用的优化算法。梯度下降算法的原理很简单，损失函数$J(\\theta)$关于参数$\\theta$的梯度将是损失函数值上升最快的方向。我们要最小化loss，只需要参数沿着梯度相反的方向前进一个步长，就可以实现损失函数的值的下降。步长$\\eta$称为学习率。参数更新公式如下：\n",
    "$$\\theta = \\theta - \\eta \\nabla J(\\theta)$$\n",
    "其中$\\nabla J(\\theta)$是参数的梯度。根据计算损失函数采用的数据量的不同，梯度下降算法又可以分为批量梯度下降算法（Batch Gradient Descent），随机梯度下降算法（Stochastic Gradient Descent）和小批量梯度下降算法（Mini-batch Gradient Descent）。批量梯度下降算法当数据量较大时会存在内存不足问题，随机梯度下降算法只针对训练集中的一个训练样本计算，小批量梯度下降算法是折中的方法，选取训练集中一个小批量的样本（一般是2的倍数，32/64/128）计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0381, 2.4327, 3.3723, 5.2480]])\n",
      "tensor([[0.9759, 1.9394, 2.8365, 3.7126]])\n",
      "tensor([[0.9992, 2.0243, 3.0262, 4.0748]])\n",
      "tensor([[0.9993, 1.9959, 2.9903, 3.9825]])\n",
      "tensor([[0.9999, 2.0014, 3.0018, 4.0045]])\n",
      "tensor([[1.0000, 1.9997, 2.9994, 3.9989]])\n",
      "tensor([[1.0000, 2.0001, 3.0001, 4.0003]])\n",
      "tensor([[1.0000, 2.0000, 3.0000, 3.9999]])\n",
      "tensor([[1.0000, 2.0000, 3.0000, 4.0000]])\n",
      "tensor([[1.0000, 2.0000, 3.0000, 4.0000]])\n",
      "tensor([[1.0000, 2.0000, 3.0000, 4.0000]])\n",
      "tensor([[1.0000, 2.0000, 3.0000, 4.0000]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1., 2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降算法代码\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "\n",
    "# 定义数据集和模型，该问题即为x有4个特征，求一个拟合关系来表征y。\n",
    "# 定义一个线性拟合网络结构作为模型，即模型为y=w1*x1+w2*x2+w3*x3+w4*x4+w0。\n",
    "# 用矩阵的形式表示即为 y = w*x^T。 x=[1,x1,x2,x3,x4]，w=[w0,w1,w2,w3,w4]\n",
    "# X中每一行为1个样本，即一共100个样本\n",
    "torch.manual_seed(1)\n",
    "X = torch.normal(0, 10, (100, 4))\n",
    "y = X @ torch.Tensor([1,2,3,4])\n",
    "y = y.view(-1, 1)\n",
    "\n",
    "# 定义网络结构\n",
    "model = nn.Linear(in_features=4, out_features=1, bias=False)\n",
    "\n",
    "# 设置学习率和迭代次数\n",
    "ln=0.01\n",
    "num = 100\n",
    "\n",
    "# 设置损失函数\n",
    "def get_loss(y, y_hat):\n",
    "    \"\"\" \n",
    "    损失函数为真实值与拟合值\n",
    "    \"\"\"\n",
    "    return torch.sum(0.5 * (y_hat - y)**2)/y.shape[0]\n",
    "\n",
    "def get_gradient(x, y, y_hat):\n",
    "    \"\"\" \n",
    "    获取梯度，这里的梯度指的时带求解的参数的梯度，即w\n",
    "    根据线性拟合的模型，损失函数的梯度\n",
    "    \"\"\"\n",
    "    return ((y_hat -y).t() @ x) /x.shape[0]\n",
    "\n",
    "# 迭代\n",
    "for i in range(num):\n",
    "    y_hat = model(X)\n",
    "    gradient = get_gradient(X, y, y_hat)\n",
    "    model.weight.data -= ln * gradient\n",
    "    loss = get_loss(y, y_hat)\n",
    "    print(model.weight.data)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b83f44245b5c9066579951731f3eeeb2eeeb5cf64b7622b7e92ada2bcaf47f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
